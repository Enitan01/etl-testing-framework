# etl-testing-framework
A lightweight, endâ€‘toâ€‘end ETL testing framework demonstrating ingestion validation, transformation testing, dataâ€‘quality checks, and sourceâ€‘toâ€‘target verification. This framework simulates a real-world data pipeline and demonstrates how to validate each stage using Python and PyTest.

## ğŸ“ Project Structure
etl-testing-framework/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source_data.csv
â”‚   â”œâ”€â”€ expected_output.csv
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ transformations.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”œâ”€â”€ test_data_quality.py
â”‚   â”œâ”€â”€ test_source_to_target.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ validators.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ¯ Purpose of This Project

This repository demonstrates:

- ETL pipeline orchestration  
- Data transformation logic  
- Data quality validation  
- Source-to-target reconciliation  
- Automated testing using PyTest  
- Reusable validation utilities  
- Clean, maintainable test architecture  

It is ideal for showcasing ETL testing skills in interviews, portfolios, and GitHub profiles.

---

## ğŸš€ How to Run the Pipeline

```bash
python -c "from etl.pipeline import run_pipeline; import pandas as pd; print(run_pipeline('data/source_data.csv'))"
```
This command loads the source CSV, applies all transformations, and prints the curated output.


## ğŸ§ª How to Run Tests

Install all dependencies

```bash
pip install -r requirements.txt
```

Run the full test suite

```bash 
pytest -v
```

Run a specific test file

```bash
pytest tests/test_transformations.py -v
```

Run a single test inside a file

```bash
pytest tests/test_data_quality.py::test_no_nulls -v
```


